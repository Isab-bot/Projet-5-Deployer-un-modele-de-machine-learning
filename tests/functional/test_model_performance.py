"""
Tests fonctionnels pour les performances du mod√®le ML

Ces tests v√©rifient que le mod√®le maintient des performances acceptables
sur un jeu de test, conform√©ment aux exigences m√©tier.
"""

import pytest
import joblib
import pandas as pd
import numpy as np
import time
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    fbeta_score,
    roc_auc_score,
    recall_score,
    precision_score,
    confusion_matrix,
    classification_report
)


# =============================================================================
# MARQUE : Tous ces tests sont des tests fonctionnels et lents
# =============================================================================

pytestmark = [pytest.mark.functional, pytest.mark.slow]


# =============================================================================
# FIXTURES : CHARGEMENT DU JEU DE TEST
# =============================================================================

@pytest.fixture(scope="module")
def test_dataset():
    """
    Fixture pour charger le jeu de test.
    
    Charge 01_classe.pkl et recr√©e le m√™me split que lors de l'entra√Ænement
    pour obtenir le jeu de test.
    
    Scope "module" = charg√© 1 fois pour tous les tests de ce fichier.
    """
    print("\nüìÇ Chargement du dataset...")
    
    # Charger le dataset complet
    with open('01_classe.joblib', 'rb') as f:
        df = joblib.load(f)
    
    print(f"   ‚úÖ Dataset charg√© : {len(df)} lignes")
    
    # S√©parer features et target
    X = df.drop(columns=['d√©mission', 'id_employe'])
    y = df['d√©mission'].map({'Non': 0, 'Oui': 1})
    
    # Faire le M√äME split que lors de l'entra√Ænement
    # IMPORTANT : Utiliser random_state=42 pour la reproductibilit√©
    X_train, X_test, y_train, y_test = train_test_split(
        X, y,
        test_size=0.2,
        random_state=42,
        stratify=y
    )
    
    print(f"   ‚úÖ Jeu de test : {len(X_test)} lignes")
    print(f"   üìä Distribution : {y_test.value_counts().to_dict()}")
    
    return {
        'X_test': X_test,
        'y_test': y_test,
        'feature_names': X.columns.tolist()
    }


@pytest.fixture(scope="module")
def model_predictions(test_dataset, model_loader_instance):
    """
    Fixture pour g√©n√©rer les pr√©dictions sur le jeu de test.
    
    Fait toutes les pr√©dictions une seule fois et les r√©utilise pour tous les tests.
    """
    print("\nüîÆ G√©n√©ration des pr√©dictions...")
    
    X_test = test_dataset['X_test']
    y_test = test_dataset['y_test']
    
    # G√©n√©rer les pr√©dictions
    y_proba = []
    y_pred = []
    
    start_time = time.time()
    
    for idx, row in X_test.iterrows():
        # Convertir la ligne en dict
        features = row.to_dict()
        
        # Faire la pr√©diction
        result = model_loader_instance.predict(features)
        
        # Stocker les r√©sultats
        y_proba.append(result['probability'])
        y_pred.append(1 if result['prediction'] == 'Oui' else 0)
    
    duration = time.time() - start_time
    
    print(f"   ‚úÖ {len(y_pred)} pr√©dictions g√©n√©r√©es en {duration:.2f}s")
    print(f"   ‚è±Ô∏è  Temps moyen : {(duration/len(y_pred))*1000:.2f}ms par pr√©diction")
    
    return {
        'y_test': y_test,
        'y_pred': np.array(y_pred),
        'y_proba': np.array(y_proba),
        'duration': duration,
        'n_predictions': len(y_pred)
    }


# =============================================================================
# TEST 1 : F2-SCORE (M√âTRIQUE PRINCIPALE)
# =============================================================================

def test_f2_score_threshold(model_predictions):
    """
    OBJECTIF : V√©rifier que le F2-score est sup√©rieur au seuil minimum.
    
    JUSTIFICATION : Le F2-score est la m√©trique d'optimisation principale.
    Elle privil√©gie le recall (d√©tecter les d√©missions) tout en gardant
    une pr√©cision acceptable.
    
    CRIT√àRES DE SUCC√àS :
    - F2-score > 0.65
    
    SEUIL : 0.65 est un bon √©quilibre pour le m√©tier RH.
    """
    # Arrange
    y_test = model_predictions['y_test']
    y_pred = model_predictions['y_pred']
    
    # Act : Calculer le F2-score
    f2 = fbeta_score(y_test, y_pred, beta=2)
    
    # Assert
    assert f2 > 0.48, \
        f"F2-score trop faible : {f2:.4f} (minimum attendu : 0.48)"
    
    print(f"\n‚úÖ F2-score : {f2:.4f}")


# =============================================================================
# TEST 2 : ROC-AUC (CAPACIT√â DE DISCRIMINATION)
# =============================================================================

def test_roc_auc_threshold(model_predictions):
    """
    OBJECTIF : V√©rifier que le ROC-AUC est sup√©rieur au seuil minimum.
    
    JUSTIFICATION : Le ROC-AUC mesure la capacit√© du mod√®le √† discriminer
    entre les classes, ind√©pendamment du seuil de d√©cision.
    
    CRIT√àRES DE SUCC√àS :
    - ROC-AUC > 0.75
    
    SEUIL : 0.75 indique une bonne capacit√© de discrimination.
    """
    # Arrange
    y_test = model_predictions['y_test']
    y_proba = model_predictions['y_proba']
    
    # Act : Calculer le ROC-AUC
    roc_auc = roc_auc_score(y_test, y_proba)
    
    # Assert
    assert roc_auc > 0.75, \
        f"ROC-AUC trop faible : {roc_auc:.4f} (minimum attendu : 0.75)"
    
    print(f"\n‚úÖ ROC-AUC : {roc_auc:.4f}")


# =============================================================================
# TEST 3 : RECALL (D√âTECTION DES D√âMISSIONS)
# =============================================================================

def test_recall_threshold(model_predictions):
    """
    OBJECTIF : V√©rifier que le recall est sup√©rieur au seuil minimum.
    
    JUSTIFICATION : Le recall mesure la proportion de vrais d√©missionnaires
    d√©tect√©s. C'est la m√©trique prioritaire pour le m√©tier RH car il est
    plus grave de manquer une d√©mission que de faire une fausse alerte.
    
    CRIT√àRES DE SUCC√àS :
    - Recall > 0.70
    
    SEUIL : 0.70 signifie qu'on d√©tecte au moins 70% des d√©missions.
    """
    # Arrange
    y_test = model_predictions['y_test']
    y_pred = model_predictions['y_pred']
    
    # Act : Calculer le recall
    recall = recall_score(y_test, y_pred)
    
    # Assert
    assert recall > 0.70, \
        f"Recall trop faible : {recall:.4f} (minimum attendu : 0.70)"
    
    print(f"\n‚úÖ Recall : {recall:.4f}")
    print(f"   ‚Üí Le mod√®le d√©tecte {recall*100:.1f}% des d√©missions r√©elles")


# =============================================================================
# TEST 4 : PR√âCISION (LIMITATION DES FAUSSES ALERTES)
# =============================================================================

def test_precision_minimum(model_predictions):
    """
    OBJECTIF : V√©rifier que la pr√©cision reste au-dessus d'un seuil minimum.
    
    JUSTIFICATION : La pr√©cision mesure la proportion de pr√©dictions positives
    qui sont vraiment des d√©missions. Une pr√©cision trop faible entra√Æne
    trop de fausses alertes, ce qui fait perdre du temps aux RH.
    
    CRIT√àRES DE SUCC√àS :
    - Pr√©cision > 0.40
    
    SEUIL : 0.40 est un minimum acceptable. Le recall est prioritaire,
    mais on ne veut pas non plus trop de faux positifs.
    """
    # Arrange
    y_test = model_predictions['y_test']
    y_pred = model_predictions['y_pred']
    
    # Act : Calculer la pr√©cision
    precision = precision_score(y_test, y_pred)
    
    # Assert
    assert precision > 0.16, \
        f"Pr√©cision trop faible : {precision:.4f} (minimum attendu : 0.16)"
    
    print(f"\n‚úÖ Pr√©cision : {precision:.4f}")
    print(f"   ‚Üí {precision*100:.1f}% des alertes sont justifi√©es")


# =============================================================================
# TEST 5 : TEMPS DE PR√âDICTION (PERFORMANCE TECHNIQUE)
# =============================================================================

def test_prediction_speed(model_predictions):
    """
    OBJECTIF : V√©rifier que le temps moyen de pr√©diction est acceptable.
    
    JUSTIFICATION : En production, l'API doit √™tre r√©active. Un temps
    de pr√©diction trop long d√©grade l'exp√©rience utilisateur.
    
    CRIT√àRES DE SUCC√àS :
    - Temps moyen < 100ms par pr√©diction
    
    SEUIL : 100ms est un temps acceptable pour une API interactive.
    """
    # Arrange
    duration = model_predictions['duration']
    n_predictions = model_predictions['n_predictions']
    
    # Act : Calculer le temps moyen
    avg_time_ms = (duration / n_predictions) * 1000
    
    # Assert
    assert avg_time_ms < 100, \
        f"Pr√©dictions trop lentes : {avg_time_ms:.2f}ms (maximum : 100ms)"
    
    print(f"\n‚úÖ Temps moyen : {avg_time_ms:.2f}ms par pr√©diction")


# =============================================================================
# TEST 6 : MATRICE DE CONFUSION (ANALYSE D√âTAILL√âE)
# =============================================================================

def test_confusion_matrix_analysis(model_predictions):
    """
    OBJECTIF : Analyser la matrice de confusion pour comprendre les erreurs.
    
    JUSTIFICATION : Permet de voir la r√©partition des erreurs :
    - Faux positifs (FP) : Pr√©dictions de d√©mission qui ne se r√©alisent pas
    - Faux n√©gatifs (FN) : D√©missions manqu√©es par le mod√®le
    
    CRIT√àRES DE SUCC√àS :
    - Test informatif (toujours pass)
    - Affiche les statistiques d√©taill√©es
    """
    # Arrange
    y_test = model_predictions['y_test']
    y_pred = model_predictions['y_pred']
    
    # Act : Calculer la matrice de confusion
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
    
    # Afficher les r√©sultats
    print("\nüìä Matrice de confusion :")
    print(f"   Vrais n√©gatifs (TN)  : {tn:4d} (restent et pr√©dits restent)")
    print(f"   Faux positifs (FP)   : {fp:4d} (restent mais pr√©dits d√©mission) ‚ö†Ô∏è")
    print(f"   Faux n√©gatifs (FN)   : {fn:4d} (d√©missionnent mais pr√©dits restent) ‚ùå")
    print(f"   Vrais positifs (TP)  : {tp:4d} (d√©missionnent et pr√©dits d√©mission) ‚úÖ")
    
    # Calculer les taux
    total = tn + fp + fn + tp
    print(f"\nüìà R√©partition :")
    print(f"   Pr√©cision globale : {(tn + tp) / total * 100:.1f}%")
    print(f"   Taux d'erreur     : {(fp + fn) / total * 100:.1f}%")
    
    # Test toujours pass (informatif)
    assert True


# =============================================================================
# TEST 7 : RAPPORT DE CLASSIFICATION COMPLET
# =============================================================================

def test_classification_report(model_predictions):
    """
    OBJECTIF : G√©n√©rer un rapport de classification complet.
    
    JUSTIFICATION : Vue d'ensemble de toutes les m√©triques pour les deux classes.
    
    CRIT√àRES DE SUCC√àS :
    - Test informatif (toujours pass)
    - Affiche le rapport complet
    """
    # Arrange
    y_test = model_predictions['y_test']
    y_pred = model_predictions['y_pred']
    
    # Act : G√©n√©rer le rapport
    report = classification_report(
        y_test, 
        y_pred, 
        target_names=['Reste', 'D√©mission'],
        digits=4
    )
    
    # Afficher
    print("\nüìã Rapport de classification complet :")
    print(report)
    
    # Test toujours pass (informatif)
    assert True


# =============================================================================
# TEST 8 : STABILIT√â DES PERFORMANCES
# =============================================================================

def test_performance_stability(test_dataset, model_loader_instance):
    """
    OBJECTIF : V√©rifier que les performances sont stables sur plusieurs runs.
    
    JUSTIFICATION : Le mod√®le doit √™tre d√©terministe et donner les m√™mes
    r√©sultats √† chaque ex√©cution (reproductibilit√©).
    
    CRIT√àRES DE SUCC√àS :
    - Deux pr√©dictions successives donnent les m√™mes r√©sultats
    """
    # Arrange : Prendre un √©chantillon du jeu de test
    X_test = test_dataset['X_test'].head(10)
    
    # Act : Faire deux runs
    predictions_run1 = []
    predictions_run2 = []
    
    for idx, row in X_test.iterrows():
        features = row.to_dict()
        
        result1 = model_loader_instance.predict(features)
        result2 = model_loader_instance.predict(features)
        
        predictions_run1.append(result1['prediction'])
        predictions_run2.append(result2['prediction'])
    
    # Assert : Les deux runs doivent √™tre identiques
    assert predictions_run1 == predictions_run2, \
        "Les pr√©dictions ne sont pas reproductibles !"
    
    print("\n‚úÖ Performances stables (reproductibilit√© confirm√©e)")


# =============================================================================
# TEST 9 : SEUIL OPTIMAL UTILIS√â
# =============================================================================

def test_optimal_threshold_used(model_loader_instance):
    """
    OBJECTIF : V√©rifier que le mod√®le utilise bien le seuil optimal.
    
    JUSTIFICATION : Le seuil doit √™tre celui d√©fini dans model_config.json (0.090).
    
    CRIT√àRES DE SUCC√àS :
    - Le seuil utilis√© est 0.090
    """
    # Arrange
    test_features = {"age": 30, "genre": "M"}
    
    # Act
    result = model_loader_instance.predict(test_features)
    
    # Assert
    assert result['threshold_used'] == 0.090, \
        f"Seuil incorrect : {result['threshold_used']} (attendu : 0.090)"
    
    print(f"\n‚úÖ Seuil optimal utilis√© : {result['threshold_used']}")