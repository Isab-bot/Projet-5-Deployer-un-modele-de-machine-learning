import pickle
import json
import pandas as pd
from sqlalchemy.orm import Session
from database import SessionLocal
from models import TrainingData
import joblib

print("üìÇ Chargement du dataset...")

# Charger le fichier pickle
with open('01_classe.joblib', 'rb') as f:
    df = joblib.load(f)

print(f"‚úÖ Dataset charg√© : {len(df)} lignes, {len(df.columns)} colonnes")
print(f"Colonnes : {df.columns.tolist()}")

# Connexion √† la base de donn√©es
db = SessionLocal()

print("\nüì• Importation dans PostgreSQL...")

# Compteur pour suivre la progression
count = 0

for index, row in df.iterrows():
    # Extraire la target (d√©mission)
    target_value = str(row['d√©mission']) if pd.notna(row['d√©mission']) else None
    
    # Cr√©er un dictionnaire avec toutes les features SAUF d√©mission
    features_dict = row.drop('d√©mission').to_dict()
    
    # Convertir les valeurs NaN en None pour JSON
    features_dict = {k: (None if pd.isna(v) else v) for k, v in features_dict.items()}
    
    # Convertir en JSON
    features_json = json.dumps(features_dict)
    
    # Cr√©er l'entr√©e dans la DB
    db_entry = TrainingData(
        identifier=f"RECORD_{index}",  # Identifiant auto-g√©n√©r√©
        features=features_json,
        target=target_value
    )
    
    db.add(db_entry)
    count += 1
    
    # Commit par batch de 100 pour optimiser
    if count % 100 == 0:
        db.commit()
        print(f"  ‚Üí {count}/{len(df)} lignes import√©es...")

# Commit final
db.commit()
db.close()

print(f"\n‚úÖ Importation termin√©e ! {count} lignes ajout√©es √† la table 'training_data'")